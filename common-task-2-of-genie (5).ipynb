{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11151123,"sourceType":"datasetVersion","datasetId":6957107}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch_geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:58:16.295970Z","iopub.execute_input":"2025-03-26T14:58:16.296280Z","iopub.status.idle":"2025-03-26T14:58:21.594990Z","shell.execute_reply.started":"2025-03-26T14:58:16.296254Z","shell.execute_reply":"2025-03-26T14:58:21.594191Z"}},"outputs":[{"name":"stdout","text":"Collecting torch_geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.11.12)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.12.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.18.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2025.1.31)\nRequirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch_geometric) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torch_geometric) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torch_geometric) (2024.2.0)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.6.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import (\n    global_mean_pool, global_max_pool, global_add_pool,\n    GATConv, EdgeConv, SAGPooling\n)\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.transforms import BaseTransform\n\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:58:21.596226Z","iopub.execute_input":"2025-03-26T14:58:21.596456Z","iopub.status.idle":"2025-03-26T14:58:29.197760Z","shell.execute_reply.started":"2025-03-26T14:58:21.596436Z","shell.execute_reply":"2025-03-26T14:58:29.196838Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class RandomPhiShift(BaseTransform):\n    def __init__(self, dphi=0.2):\n        self.dphi = dphi\n    \n    def __call__(self, data):\n        if data.pos.size(0) > 0:\n            shift = (torch.rand(1) * 2 - 1) * self.dphi\n            data.pos[:, 1] = data.pos[:, 1] + shift\n            data.pos[:, 1] = torch.atan2(torch.sin(data.pos[:, 1]), torch.cos(data.pos[:, 1]))\n        return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:58:29.199234Z","iopub.execute_input":"2025-03-26T14:58:29.199630Z","iopub.status.idle":"2025-03-26T14:58:29.204529Z","shell.execute_reply.started":"2025-03-26T14:58:29.199601Z","shell.execute_reply":"2025-03-26T14:58:29.203721Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class DynamicHybridGraph(nn.Module):\n    def __init__(self, k=4, alpha=0.5, beta=0.5):\n        super().__init__()\n        self.k = k\n        self.alpha = alpha\n        self.beta = beta\n        \n    def forward(self, x, pos, batch):\n        device = x.device\n        batch_size = batch.max().item() + 1\n        edge_index_list = []\n        edge_attr_list = []\n        \n        for i in range(batch_size):\n            mask = (batch == i)\n            x_batch = x[mask]\n            pos_batch = pos[mask]\n            if x_batch.size(0) <= 1:\n                continue\n            \n            # Compute distances\n            dist_geom = torch.cdist(pos_batch, pos_batch)\n            dist_feat = torch.cdist(x_batch, x_batch)\n            dist_combined = self.alpha * dist_geom + self.beta * dist_feat\n            \n            k_eff = min(self.k + 1, x_batch.size(0))\n            _, topk_indices = torch.topk(dist_combined, k=k_eff, largest=False, dim=1)\n            topk_indices = topk_indices[:, 1:]  # remove self-loop\n            \n            rows = torch.arange(x_batch.size(0), device=device).view(-1, 1).repeat(1, k_eff-1)\n            offset = mask.nonzero(as_tuple=True)[0].min().item()\n            edge_index = torch.stack([rows.reshape(-1), topk_indices.reshape(-1)], dim=0) + offset\n            \n            source_nodes = edge_index[0] - offset\n            target_nodes = edge_index[1] - offset\n            delta_eta = pos_batch[target_nodes, 0] - pos_batch[source_nodes, 0]\n            delta_phi = pos_batch[target_nodes, 1] - pos_batch[source_nodes, 1]\n            delta_phi = torch.atan2(torch.sin(delta_phi), torch.cos(delta_phi))\n            delta_r = torch.sqrt(delta_eta**2 + delta_phi**2)\n            delta_embed = x_batch[target_nodes] - x_batch[source_nodes]\n            delta_embed_norm = torch.norm(delta_embed, p=2, dim=1, keepdim=True)\n            edge_attr = torch.cat([\n                delta_eta.unsqueeze(1),\n                delta_phi.unsqueeze(1),\n                delta_r.unsqueeze(1),\n                delta_embed_norm\n            ], dim=1)\n            \n            edge_index_list.append(edge_index)\n            edge_attr_list.append(edge_attr)\n        \n        if not edge_index_list:\n            return (torch.zeros((2, 0), device=device, dtype=torch.long),\n                    torch.zeros((0, 4), device=device))\n        edge_index = torch.cat(edge_index_list, dim=1)\n        edge_attr = torch.cat(edge_attr_list, dim=0)\n        return edge_index, edge_attr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:58:29.206025Z","iopub.execute_input":"2025-03-26T14:58:29.206374Z","iopub.status.idle":"2025-03-26T14:58:29.230706Z","shell.execute_reply.started":"2025-03-26T14:58:29.206342Z","shell.execute_reply":"2025-03-26T14:58:29.229962Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def create_multiscale_knn(pos, k1=4, k2=6):\n    def knn_graph(pos, k):\n        n = pos.size(0)\n        if n <= 1:\n            return torch.zeros((2, 0), dtype=torch.long), torch.zeros((0, 3), dtype=torch.float)\n        dist = torch.cdist(pos, pos)\n        _, nn_idx = torch.topk(dist, k=min(k+1, n), dim=1, largest=False)\n        nn_idx = nn_idx[:, 1:]\n        rows = torch.arange(n).view(-1, 1).repeat(1, min(k, n-1))\n        edge_index = torch.stack([rows.reshape(-1), nn_idx.reshape(-1)], dim=0)\n        source_nodes = edge_index[0]\n        target_nodes = edge_index[1]\n        delta_eta = pos[target_nodes, 0] - pos[source_nodes, 0]\n        delta_phi = pos[target_nodes, 1] - pos[source_nodes, 1]\n        delta_phi = torch.atan2(torch.sin(delta_phi), torch.cos(delta_phi))\n        delta_r = torch.sqrt(delta_eta**2 + delta_phi**2)\n        edge_attr = torch.stack([delta_eta, delta_phi, delta_r], dim=1)\n        return edge_index, edge_attr\n    eidx_s, eattr_s = knn_graph(pos, k1)\n    eidx_l, eattr_l = knn_graph(pos, k2)\n    return eidx_s, eattr_s, eidx_l, eattr_l","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:58:29.231563Z","iopub.execute_input":"2025-03-26T14:58:29.231787Z","iopub.status.idle":"2025-03-26T14:58:29.251500Z","shell.execute_reply.started":"2025-03-26T14:58:29.231758Z","shell.execute_reply":"2025-03-26T14:58:29.250761Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class JetGraphProcessor:\n    def __init__(self, input_dir, output_dir, transform=None, k=8, min_energy_threshold=1e-4):\n        self.input_dir = input_dir\n        self.output_dir = output_dir\n        self.transform = transform\n        self.k = k\n        self.min_energy_threshold = min_energy_threshold\n        os.makedirs(output_dir, exist_ok=True)\n    \n    def _convert_to_graph(self, jet_image, label, m0, pt):\n        jet_image_2d = np.sum(jet_image, axis=2)\n        padded = np.pad(jet_image_2d, pad_width=1, mode='constant', constant_values=0)\n        non_zero_indices = np.where(jet_image_2d > self.min_energy_threshold)\n        points = []\n        features = []\n        for i, j in zip(non_zero_indices[0], non_zero_indices[1]):\n            pixel_eta = (i / 125.0 * 2 - 1) * 0.8\n            pixel_phi = (j / 125.0 * 2 - 1) * 0.8\n            energy_ecal = jet_image[i, j, 0]\n            energy_hcal = jet_image[i, j, 1]\n            energy_tracks = jet_image[i, j, 2]\n            total_energy = energy_ecal + energy_hcal + energy_tracks\n            pt_fraction = total_energy / (pt + 1e-9)\n            charged_fraction = energy_tracks / total_energy if total_energy > 0 else 0.0\n            local_sum = np.sum(padded[i:i+3, j:j+3])\n            angle_center = np.arctan2(j - 62.5, i - 62.5)\n            norm_dist_center = np.sqrt((i - 62.5)**2 + (j - 62.5)**2) / 62.5\n            features.append([\n                total_energy, energy_ecal, energy_hcal, energy_tracks,\n                pt_fraction, charged_fraction, local_sum,\n                pixel_eta, pixel_phi, np.log1p(total_energy),\n                np.sqrt(total_energy), angle_center, norm_dist_center\n            ])\n            points.append([pixel_eta, pixel_phi])\n        if len(points) == 0:\n            points = [[0, 0]]\n            features = [[0.0]*13]\n        x = torch.tensor(features, dtype=torch.float)\n        pos = torch.tensor(points, dtype=torch.float)\n        eidx_s, eattr_s, eidx_l, eattr_l = create_multiscale_knn(pos, k1=8, k2=16)\n        data = Data(\n            x=x,\n            pos=pos,\n            edge_index=eidx_s,\n            edge_attr=eattr_s,\n            y=torch.tensor([label], dtype=torch.long)\n        )\n        data.edge_index_large = eidx_l\n        data.edge_attr_large = eattr_l\n        data.global_features = torch.tensor([m0, pt], dtype=torch.float).unsqueeze(0)\n        return data\n    \n    def process_chunks(self, chunk_files):\n        total_graphs = 0\n        for chunk_file in chunk_files:\n            print(f\"Processing {chunk_file} ...\")\n            parts = chunk_file.replace('chunk_', '').replace('.npz', '').split('_')\n            if len(parts) >= 2:\n                start_idx, end_idx = int(parts[0]), int(parts[1])\n            else:\n                start_idx, end_idx = 0, 0\n            data_npz = np.load(os.path.join(self.input_dir, chunk_file))\n            X_jets = data_npz['X_jets']\n            y = data_npz['y']\n            m0 = data_npz['m0']\n            pt = data_npz['pt']\n            graph_list = []\n            for i in tqdm(range(len(X_jets)), desc=f\"Processing {chunk_file}\"):\n                graph = self._convert_to_graph(X_jets[i], int(y[i]), m0[i], pt[i])\n                if self.transform:\n                    graph = self.transform(graph)\n                graph_list.append(graph)\n            output_file = os.path.join(self.output_dir, f\"processed_{chunk_file.replace('.npz', '.pt')}\")\n            torch.save(graph_list, output_file)\n            total_graphs += len(graph_list)\n        print(f\"Processed {total_graphs} graphs from {len(chunk_files)} chunks.\")\n        return total_graphs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:58:29.252271Z","iopub.execute_input":"2025-03-26T14:58:29.252558Z","iopub.status.idle":"2025-03-26T14:58:29.273553Z","shell.execute_reply.started":"2025-03-26T14:58:29.252528Z","shell.execute_reply":"2025-03-26T14:58:29.272890Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class ChunkedJetGraphDataset(Dataset):\n    def __init__(self, processed_dir, transform=None):\n        super().__init__()\n        self.processed_dir = processed_dir\n        self.transform = transform\n        self.processed_files = sorted([f for f in os.listdir(processed_dir) if f.startswith('processed_chunk_')])\n        self._create_index_mapping()\n        \n    def _create_index_mapping(self):\n        self.chunk_sizes = []\n        self.chunk_indices = []\n        start_idx = 0\n        for file in self.processed_files:\n            file_path = os.path.join(self.processed_dir, file)\n            if os.path.exists(file_path):\n                try:\n                    graphs = torch.load(file_path)\n                    chunk_size = len(graphs)\n                    self.chunk_sizes.append(chunk_size)\n                    end_idx = start_idx + chunk_size\n                    self.chunk_indices.append((start_idx, end_idx, file))\n                    start_idx = end_idx\n                    del graphs\n                except Exception as e:\n                    print(f\"Error loading {file}: {e}\")\n        self.total_size = sum(self.chunk_sizes)\n        \n    def __len__(self):\n        return self.total_size\n    \n    def __getitem__(self, idx):\n        for start_idx, end_idx, file in self.chunk_indices:\n            if start_idx <= idx < end_idx:\n                graphs = torch.load(os.path.join(self.processed_dir, file))\n                local_idx = idx - start_idx\n                graph = graphs[local_idx]\n                if self.transform:\n                    graph = self.transform(graph)\n                return graph\n        raise IndexError(f\"Index {idx} out of range\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:58:29.274325Z","iopub.execute_input":"2025-03-26T14:58:29.274560Z","iopub.status.idle":"2025-03-26T14:58:29.294769Z","shell.execute_reply.started":"2025-03-26T14:58:29.274542Z","shell.execute_reply":"2025-03-26T14:58:29.293937Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class CoordinateEmbedding(nn.Module):\n   \n    def __init__(self, in_dim=2, out_dim=4):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(in_dim, out_dim),\n            nn.ReLU(),\n            nn.Linear(out_dim, out_dim)\n        )\n    \n    def forward(self, coords):\n        return self.mlp(coords)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:58:29.296889Z","iopub.execute_input":"2025-03-26T14:58:29.297152Z","iopub.status.idle":"2025-03-26T14:58:29.313727Z","shell.execute_reply.started":"2025-03-26T14:58:29.297126Z","shell.execute_reply":"2025-03-26T14:58:29.313056Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class EnhancedJetGNN(nn.Module):\n    def __init__(self, node_dim, global_dim, hidden_dim=64, out_channels=2, \n                 num_layers=3, dropout=0.3, use_dynamic_graph=True, heads=4, \n                 use_sagpool=True, sagpool_ratio=0.5):\n        super().__init__()\n        self.node_dim = node_dim\n        self.global_dim = global_dim\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.use_dynamic_graph = use_dynamic_graph\n        self.heads = heads\n        self.use_sagpool = use_sagpool\n        self.sagpool_ratio = sagpool_ratio\n        \n        # Coordinate embedding for (η, φ) at indices 7 and 8\n        self.coord_embed = CoordinateEmbedding(in_dim=2, out_dim=4)\n        # Process the remaining node features\n        self.feature_mlp = nn.Sequential(\n            nn.Linear(node_dim - 2, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim)\n        )\n        # Combine coordinate embedding with processed features\n        self.comb_mlp = nn.Sequential(\n            nn.Linear(hidden_dim + 4, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n        # Global feature encoder\n        self.global_encoder = nn.Sequential(\n            nn.Linear(global_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n        # Dynamic adjacency module\n        if use_dynamic_graph:\n            self.dynamic_graph = DynamicHybridGraph(k=8, alpha=0.5, beta=0.5)\n        # GNN layers (alternating GAT and EdgeConv)\n        self.gnn_layers = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n        self.gnn_layers.append(\n            GATConv(hidden_dim, hidden_dim // heads, heads=heads, edge_dim=4, dropout=dropout)\n        )\n        self.batch_norms.append(BatchNorm(hidden_dim))\n        for i in range(1, num_layers):\n            if i % 2 == 1:\n                self.gnn_layers.append(\n                    EdgeConv(\n                        nn=nn.Sequential(\n                            nn.Linear(hidden_dim*2, hidden_dim),\n                            nn.ReLU(),\n                            nn.Dropout(dropout),\n                            nn.Linear(hidden_dim, hidden_dim)\n                        ),\n                        aggr='mean'\n                    )\n                )\n            else:\n                self.gnn_layers.append(\n                    GATConv(hidden_dim, hidden_dim // heads, heads=heads, edge_dim=4, dropout=dropout)\n                )\n            self.batch_norms.append(BatchNorm(hidden_dim))\n        # Optional SAGPooling for hierarchical pooling\n        if self.use_sagpool:\n            self.sagpool = SAGPooling(hidden_dim, ratio=self.sagpool_ratio)\n        # Final classifier: multi-scale pooling (mean, max, sum) combined with global features\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_dim*3 + hidden_dim, hidden_dim*2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim*2, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, out_channels)\n        )\n    \n    def forward(self, data):\n        x, batch = data.x, data.batch\n        edge_index, edge_attr = data.edge_index, data.edge_attr\n        pos = data.pos\n        if hasattr(data, 'global_features'):\n            global_features = data.global_features\n        else:\n            global_features = torch.zeros((batch.max().item()+1, self.global_dim), device=x.device)\n        if batch is None:\n            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n        \n        # Separate coordinate features (assumed at indices 7 and 8)\n        coords = x[:, [7, 8]]\n        other_feats = torch.cat([x[:, :7], x[:, 9:]], dim=1)\n        coord_emb = self.coord_embed(coords)\n        feat_emb = self.feature_mlp(other_feats)\n        x = torch.cat([feat_emb, coord_emb], dim=1)\n        x = self.comb_mlp(x)\n        global_x = self.global_encoder(global_features)\n        \n        for i, conv in enumerate(self.gnn_layers):\n            if self.use_dynamic_graph:\n                edge_index, edge_attr = self.dynamic_graph(x, pos, batch)\n            if isinstance(conv, GATConv):\n                x = conv(x, edge_index, edge_attr=edge_attr)\n            else:\n                x = conv(x, edge_index)\n            x = self.batch_norms[i](x)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        \n        if self.use_sagpool:\n            x, edge_index, edge_attr, batch, _, _ = self.sagpool(x, edge_index, edge_attr, batch=batch)\n        pooled_mean = global_mean_pool(x, batch)\n        pooled_max = global_max_pool(x, batch)\n        pooled_sum = global_add_pool(x, batch)\n        combined = torch.cat([pooled_mean, pooled_max, pooled_sum, global_x], dim=1)\n        out = self.classifier(combined)\n        return F.log_softmax(out, dim=-1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LabelSmoothingLoss(nn.Module):\n    def __init__(self, smoothing=0.1, reduction='mean'):\n        super().__init__()\n        self.smoothing = smoothing\n        self.reduction = reduction\n    \n    def forward(self, pred, target):\n        num_classes = pred.size(1)\n        with torch.no_grad():\n            true_dist = pred.new_ones(pred.size()) * (self.smoothing / (num_classes - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n        loss = -true_dist * pred\n        loss = loss.sum(dim=1)\n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()\n        else:\n            return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:58:29.338175Z","iopub.execute_input":"2025-03-26T14:58:29.338366Z","iopub.status.idle":"2025-03-26T14:58:29.357216Z","shell.execute_reply.started":"2025-03-26T14:58:29.338349Z","shell.execute_reply":"2025-03-26T14:58:29.356596Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class EnhancedJetGNNTrainer:\n    def __init__(self, model, device, lr=1e-3, weight_decay=5e-4, smoothing=0.1):\n        self.model = model.to(device)\n        self.device = device\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n        self.criterion = LabelSmoothingLoss(smoothing=smoothing).to(device)\n    \n    def train(self, train_loader, val_loader, num_epochs=50, patience=10, model_save_path='best_jet_gnn_model.pt'):\n        best_val_loss = float('inf')\n        wait = 0\n        train_losses = []\n        val_metrics = []\n        for epoch in range(num_epochs):\n            self.model.train()\n            running_loss = 0.0\n            for data in train_loader:\n                data = data.to(self.device)\n                self.optimizer.zero_grad()\n                out = self.model(data)\n                loss = self.criterion(out, data.y.view(-1))\n                loss.backward()\n                self.optimizer.step()\n                running_loss += loss.item() * data.num_graphs\n            epoch_train_loss = running_loss / len(train_loader.dataset)\n            val_loss, val_acc, val_f1, val_auc = self.evaluate(val_loader)\n            train_losses.append(epoch_train_loss)\n            val_metrics.append({'loss': val_loss, 'accuracy': val_acc, 'f1': val_f1, 'auc': val_auc})\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {epoch_train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}\")\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                torch.save(self.model.state_dict(), model_save_path)\n                wait = 0\n            else:\n                wait += 1\n                if wait >= patience:\n                    print(\"Early stopping triggered.\")\n                    break\n        self.model.load_state_dict(torch.load(model_save_path))\n        return train_losses, val_metrics\n    \n    def evaluate(self, loader, return_predictions=False):\n        self.model.eval()\n        correct = 0\n        total = 0\n        y_true, y_pred, y_score = [], [], []\n        loss_sum = 0.0\n        with torch.no_grad():\n            for data in loader:\n                data = data.to(self.device)\n                out = self.model(data)\n                loss = self.criterion(out, data.y.view(-1))\n                loss_sum += loss.item() * data.num_graphs\n                pred = out.argmax(dim=-1)\n                correct += pred.eq(data.y.view(-1)).sum().item()\n                total += data.num_graphs\n                y_true.append(data.y.view(-1).cpu().numpy())\n                y_pred.append(pred.cpu().numpy())\n                prob_class1 = out.exp()[:, 1].cpu().numpy()\n                y_score.append(prob_class1)\n        y_true = np.concatenate(y_true, axis=0)\n        y_pred = np.concatenate(y_pred, axis=0)\n        y_score = np.concatenate(y_score, axis=0)\n        val_loss = loss_sum / len(loader.dataset)\n        accuracy = (correct / total) if total > 0 else 0\n        from sklearn.metrics import f1_score, roc_auc_score\n        f1 = f1_score(y_true, y_pred, average='binary')\n        try:\n            auc = roc_auc_score(y_true, y_score)\n        except:\n            auc = 0.0\n        if return_predictions:\n            return {'loss': val_loss, 'accuracy': accuracy, 'f1': f1, 'auc': auc}, (y_true, y_pred, y_score)\n        else:\n            return val_loss, accuracy, f1, auc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:58:29.357939Z","iopub.execute_input":"2025-03-26T14:58:29.358240Z","iopub.status.idle":"2025-03-26T14:58:29.376742Z","shell.execute_reply.started":"2025-03-26T14:58:29.358212Z","shell.execute_reply":"2025-03-26T14:58:29.376138Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def main():\n    config = {\n        'input_dir': '/kaggle/input/genie-extracted-dataset',\n        'processed_dir': '/kaggle/working/processed_jet_graphs',\n        'batch_size': 64,\n        'hidden_channels': 64,\n        'num_layers': 3,\n        'dropout': 0.3,\n        'learning_rate': 0.001,\n        'weight_decay': 5e-4,\n        'epochs': 50,\n        'patience': 10,\n        'test_size': 0.1,\n        'val_size': 0.1,\n        'use_dynamic_graph': True,\n        'gat_heads': 4,\n        'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n        'use_sagpool': True,\n        'sagpool_ratio': 0.5\n    }\n    \n    print(f\"Using device: {config['device']}\")\n    \n    # Define the chunks to skip.\n    skipped_chunks = {\n        \"chunk_0_10000.npz\",\n        \"chunk_100000_110000.npz\",\n        \"chunk_10000_20000.npz\",\n        \"chunk_110000_120000.npz\"\n    }\n    \n    # List all .npz chunk files from the input directory, skipping the ones we don't want.\n    all_chunk_files = sorted([f for f in os.listdir(config['input_dir']) \n                               if f.endswith('.npz') and f not in skipped_chunks])\n    \n    if len(all_chunk_files) < 2:\n        raise ValueError(\"Need at least 2 chunk files for training and inference separation.\")\n    \n    os.makedirs(config['processed_dir'], exist_ok=True)\n    # Process and save all chunks except the last one.\n    if not os.path.exists(config['processed_dir']) or len(os.listdir(config['processed_dir'])) == 0:\n        print(\"Processing data into graph representations for training...\")\n        processor = JetGraphProcessor(\n            config['input_dir'],\n            config['processed_dir'],\n            transform=RandomPhiShift(dphi=0.2),\n            k=8,\n            min_energy_threshold=1e-4\n        )\n        # Process all chunks except the last one\n        processor.process_chunks(all_chunk_files[:-1])\n    else:\n        print(f\"Using existing processed graphs from {config['processed_dir']}\")\n    \n    # Create dataset from the processed (saved) chunks.\n    dataset = ChunkedJetGraphDataset(config['processed_dir'])\n    print(f\"Total dataset size: {len(dataset)}\")\n    \n    indices = list(range(len(dataset)))\n    train_idx, test_idx = train_test_split(indices, test_size=config['test_size'], random_state=42)\n    train_idx, val_idx = train_test_split(train_idx, test_size=config['val_size']/(1 - config['test_size']), random_state=42)\n    \n    train_dataset = torch.utils.data.Subset(dataset, train_idx)\n    val_dataset   = torch.utils.data.Subset(dataset, val_idx)\n    test_dataset  = torch.utils.data.Subset(dataset, test_idx)\n    \n    print(f\"Train size: {len(train_dataset)}\")\n    print(f\"Validation size: {len(val_dataset)}\")\n    print(f\"Test size: {len(test_dataset)}\")\n    \n    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n    val_loader   = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n    test_loader  = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n    \n    sample_data = dataset[0]\n    node_dim = sample_data.x.size(1)\n    global_dim = sample_data.global_features.size(1) if hasattr(sample_data, 'global_features') else 2\n    \n    model = EnhancedJetGNN(\n        node_dim=node_dim,\n        global_dim=global_dim,\n        hidden_dim=config['hidden_channels'],\n        out_channels=2,\n        num_layers=config['num_layers'],\n        dropout=config['dropout'],\n        use_dynamic_graph=config['use_dynamic_graph'],\n        heads=config['gat_heads'],\n        use_sagpool=config['use_sagpool'],\n        sagpool_ratio=config['sagpool_ratio']\n    ).to(config['device'])\n    \n    trainer = EnhancedJetGNNTrainer(\n        model=model,\n        device=config['device'],\n        lr=config['learning_rate'],\n        weight_decay=config['weight_decay'],\n        smoothing=0.1\n    )\n    \n    # Train and save best model weights after all epochs.\n    train_losses, val_metrics = trainer.train(\n        train_loader=train_loader,\n        val_loader=val_loader,\n        num_epochs=config['epochs'],\n        patience=config['patience'],\n        model_save_path='best_jet_gnn_model.pt'\n    )\n    \n    test_metrics, (y_true, y_pred, y_score) = trainer.evaluate(test_loader, return_predictions=True)\n    print(\"\\nTest Set Metrics:\")\n    print(f\"Accuracy:  {test_metrics['accuracy']:.4f}\")\n    print(f\"F1 Score:  {test_metrics['f1']:.4f}\")\n    print(f\"AUC:       {test_metrics['auc']:.4f}\")\n    \n    # Inference on the last chunk (which was not saved)\n    last_chunk_file = all_chunk_files[-1]\n    print(f\"Running inference on the last chunk: {last_chunk_file}\")\n    data_npz = np.load(os.path.join(config['input_dir'], last_chunk_file))\n    X_jets = data_npz['X_jets']\n    y_last = data_npz['y']\n    m0_last = data_npz['m0']\n    pt_last = data_npz['pt']\n    last_chunk_graphs = []\n    processor = JetGraphProcessor(\n        config['input_dir'],\n        config['processed_dir'],\n        transform=RandomPhiShift(dphi=0.2),\n        k=8,\n        min_energy_threshold=1e-4\n    )\n    for i in tqdm(range(len(X_jets)), desc=f\"Processing last chunk {last_chunk_file} for inference\"):\n        graph = processor._convert_to_graph(X_jets[i], int(y_last[i]), m0_last[i], pt_last[i])\n        if processor.transform:\n            graph = processor.transform(graph)\n        last_chunk_graphs.append(graph)\n    last_chunk_loader = DataLoader(last_chunk_graphs, batch_size=config['batch_size'], shuffle=False)\n    last_chunk_metrics, (lc_y_true, lc_y_pred, lc_y_score) = trainer.evaluate(last_chunk_loader, return_predictions=True)\n    print(\"\\nLast Chunk Inference Metrics:\")\n    print(f\"Accuracy:  {last_chunk_metrics['accuracy']:.4f}\")\n    print(f\"F1 Score:  {last_chunk_metrics['f1']:.4f}\")\n    print(f\"AUC:       {last_chunk_metrics['auc']:.4f}\")\n    \n    # Plot training history\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label='Train Loss')\n    plt.plot([m['loss'] for m in val_metrics], label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot([m['accuracy'] for m in val_metrics], label='Accuracy')\n    plt.plot([m['f1'] for m in val_metrics], label='F1')\n    plt.plot([m['auc'] for m in val_metrics], label='AUC')\n    plt.xlabel('Epoch')\n    plt.ylabel('Score')\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig('training_history.png')\n    plt.show()\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:58:29.377468Z","iopub.execute_input":"2025-03-26T14:58:29.377681Z","iopub.status.idle":"2025-03-26T15:17:18.397141Z","shell.execute_reply.started":"2025-03-26T14:58:29.377663Z","shell.execute_reply":"2025-03-26T15:17:18.395899Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nProcessing data into graph representations for training...\nProcessing chunk_0_10000.npz ...\n","output_type":"stream"},{"name":"stderr","text":"Processing chunk_0_10000.npz: 100%|██████████| 10000/10000 [03:54<00:00, 42.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk_100000_110000.npz ...\n","output_type":"stream"},{"name":"stderr","text":"Processing chunk_100000_110000.npz: 100%|██████████| 10000/10000 [04:06<00:00, 40.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk_10000_20000.npz ...\n","output_type":"stream"},{"name":"stderr","text":"Processing chunk_10000_20000.npz: 100%|██████████| 10000/10000 [03:53<00:00, 42.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk_110000_120000.npz ...\n","output_type":"stream"},{"name":"stderr","text":"Processing chunk_110000_120000.npz: 100%|██████████| 10000/10000 [04:09<00:00, 40.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk_120000_130000.npz ...\n","output_type":"stream"},{"name":"stderr","text":"Processing chunk_120000_130000.npz:  39%|███▉      | 3875/10000 [01:37<02:33, 39.90it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-eaedb2d50947>\u001b[0m in \u001b[0;36m<cell line: 151>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-eaedb2d50947>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mmin_energy_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         )\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_chunk_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Using existing processed graphs from {config['processed_dir']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-7f2ed77bff54>\u001b[0m in \u001b[0;36mprocess_chunks\u001b[0;34m(self, chunk_files)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mgraph_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_jets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Processing {chunk_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_jets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-7f2ed77bff54>\u001b[0m in \u001b[0;36m_convert_to_graph\u001b[0;34m(self, jet_image, label, m0, pt)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mlocal_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mangle_center\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marctan2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m62.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m62.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mnorm_dist_center\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m62.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m62.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m62.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             features.append([\n\u001b[1;32m     35\u001b[0m                 \u001b[0mtotal_energy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy_ecal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy_hcal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy_tracks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":12}]}