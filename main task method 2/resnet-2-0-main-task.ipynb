{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, glob\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T12:53:38.180549Z","iopub.execute_input":"2025-04-01T12:53:38.180981Z","iopub.status.idle":"2025-04-01T12:53:46.579670Z","shell.execute_reply.started":"2025-04-01T12:53:38.180933Z","shell.execute_reply":"2025-04-01T12:53:46.578517Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def get_image_files(folder):\n    # Return sorted list of image file paths\n    return sorted(glob.glob(os.path.join(folder, \"*.*\")))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_chunk(file_list, chunk_index, total_chunks=14):\n    n = len(file_list)\n    chunk_size = n // total_chunks\n    start = chunk_index * chunk_size\n    if chunk_index == total_chunks - 1:\n        end = n\n    else:\n        end = start + chunk_size\n    return file_list[start:end]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def read_image(path, target_size=(32,32)):\n    # Read image using cv2, convert BGR->RGB, and resize\n    img = cv2.imread(path)\n    if img is None:\n        raise FileNotFoundError(f\"Image not found: {path}\")\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, target_size)\n    # Normalize pixel values to [0,1]\n    return img.astype(np.float32) / 255.0\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ImageStatsTransform:\n    def __call__(self, x):\n        # x: (C, H, W) torch.Tensor\n        mean = x.mean(dim=[1,2], keepdim=True)\n        std = x.std(dim=[1,2], keepdim=True) + 1e-6\n        return (x - mean) / std","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_physics_features_tensor(x, eps=1e-6):\n    # x: (B, 3, H, W) where channels represent [ecal, hcal, tracks]\n    ecal = x[:, 0:1, :, :]\n    hcal = x[:, 1:2, :, :]\n    tracks = x[:, 2:3, :, :]\n    ratio = torch.mean(ecal / (hcal + eps), dim=[2,3])\n    mean_tracks = torch.mean(tracks, dim=[2,3])\n    diff = torch.mean(ecal - hcal, dim=[2,3])\n    norm_diff = torch.mean(torch.abs(ecal - hcal) / (ecal + hcal + eps), dim=[2,3])\n    return torch.cat([ratio, mean_tracks, diff, norm_diff], dim=1)  # (B, 4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T12:53:46.587911Z","iopub.execute_input":"2025-04-01T12:53:46.588428Z","iopub.status.idle":"2025-04-01T12:53:46.613808Z","shell.execute_reply.started":"2025-04-01T12:53:46.588389Z","shell.execute_reply":"2025-04-01T12:53:46.612739Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class ChannelWiseFPN(nn.Module):\n    def __init__(self, in_channels=3, out_channels=16):\n        super(ChannelWiseFPN, self).__init__()\n        self.conv1x1 = nn.Conv2d(1, out_channels, kernel_size=1, padding=0)\n        self.conv3x3 = nn.Conv2d(1, out_channels, kernel_size=3, padding=1)\n        self.conv5x5 = nn.Conv2d(1, out_channels, kernel_size=5, padding=2)\n        self.fuse_conv = nn.Sequential(\n            nn.Conv2d(3*out_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(out_channels, out_channels, kernel_size=1)\n        )\n    def forward(self, x):\n        # x: (B, in_channels, H, W)\n        outputs = []\n        for i in range(x.shape[1]):\n            xi = x[:, i:i+1, :, :]\n            feat1 = self.conv1x1(xi)\n            feat2 = self.conv3x3(xi)\n            feat3 = self.conv5x5(xi)\n            cat_feats = torch.cat([feat1, feat2, feat3], dim=1)\n            fused = self.fuse_conv(cat_feats)\n            outputs.append(fused)\n        return torch.cat(outputs, dim=1)  # (B, in_channels*out_channels, H, W)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T12:53:46.616540Z","iopub.execute_input":"2025-04-01T12:53:46.616862Z","iopub.status.idle":"2025-04-01T12:53:46.632599Z","shell.execute_reply.started":"2025-04-01T12:53:46.616834Z","shell.execute_reply":"2025-04-01T12:53:46.631569Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class QuarkGluonDataset(Dataset):\n    def __init__(self, orig_files, rec_files, graph_files, transform=None):\n        # Lists of file paths (assumed same order for corresponding samples)\n        self.orig_files = orig_files\n        self.rec_files = rec_files\n        self.graph_files = graph_files\n        self.transform = transform\n        # For demonstration, labels are randomly assigned. Replace with actual labels.\n        self.labels = np.random.randint(0,2,size=(len(orig_files),))\n    def __len__(self):\n        return len(self.labels)\n    def __getitem__(self, idx):\n        orig = read_image(self.orig_files[idx])\n        rec = read_image(self.rec_files[idx])\n        graph = read_image(self.graph_files[idx])\n        # Convert to tensor and rearrange to (C, H, W)\n        orig = torch.tensor(orig).permute(2,0,1)\n        rec = torch.tensor(rec).permute(2,0,1)\n        graph = torch.tensor(graph).permute(2,0,1)\n        if self.transform:\n            orig = self.transform(orig)\n            rec = self.transform(rec)\n            graph = self.transform(graph)\n        label = self.labels[idx]\n        return {\"original\": orig, \"reconstructed\": rec, \"graph\": graph, \"label\": label}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T12:53:46.634059Z","iopub.execute_input":"2025-04-01T12:53:46.634457Z","iopub.status.idle":"2025-04-01T12:53:46.657642Z","shell.execute_reply.started":"2025-04-01T12:53:46.634426Z","shell.execute_reply":"2025-04-01T12:53:46.656427Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def load_data_chunk(chunk_index):\n    # Dummy loader: replace with your actual data loader.\n    num_samples = 100\n    H, W = 32, 32\n    original = np.random.rand(num_samples, 3, H, W).astype(np.float32)\n    reconstructed = np.random.rand(num_samples, 3, H, W).astype(np.float32)\n    graph = np.random.rand(num_samples, 3, H, W).astype(np.float32)\n    labels = np.random.randint(0, 2, size=(num_samples,))\n    return original, reconstructed, graph, labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T12:53:46.658655Z","iopub.execute_input":"2025-04-01T12:53:46.659016Z","iopub.status.idle":"2025-04-01T12:53:46.675974Z","shell.execute_reply.started":"2025-04-01T12:53:46.658977Z","shell.execute_reply":"2025-04-01T12:53:46.674997Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class ResNetWithChannelFPN(nn.Module):\n    def __init__(self, num_classes=2):\n        super(ResNetWithChannelFPN, self).__init__()\n        # Three FPN modules for original, difference, and graph.\n        self.fpn_orig = ChannelWiseFPN(3, 16)\n        self.fpn_diff = ChannelWiseFPN(3, 16)\n        self.fpn_graph = ChannelWiseFPN(3, 16)\n        # Three ResNet18 backbones (pretrained), modified to accept 48 channels.\n        self.resnet_orig = models.resnet18(pretrained=True)\n        self.resnet_diff = models.resnet18(pretrained=True)\n        self.resnet_graph = models.resnet18(pretrained=True)\n        for branch in [self.resnet_orig, self.resnet_diff, self.resnet_graph]:\n            branch.conv1 = nn.Conv2d(48, 64, kernel_size=7, stride=2, padding=3, bias=False)\n            branch.fc = nn.Identity()  # Feature output: 512-dim.\n        # Final classifier: fuse deep features (512) with physics features (4).\n        self.classifier = nn.Linear(512 + 4, num_classes)\n    def forward(self, x_orig, x_rec, x_graph):\n        # Compute difference image.\n        x_diff = x_orig - x_rec\n        feat_orig = self.resnet_orig(self.fpn_orig(x_orig))\n        feat_diff = self.resnet_diff(self.fpn_diff(x_diff))\n        feat_graph = self.resnet_graph(self.fpn_graph(x_graph))\n        deep_feat = (feat_orig + feat_diff + feat_graph) / 3.0\n        phys_feat = compute_physics_features_tensor(x_orig)\n        combined_feat = torch.cat([deep_feat, phys_feat], dim=1)\n        logits = self.classifier(combined_feat)\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T12:53:46.677084Z","iopub.execute_input":"2025-04-01T12:53:46.677581Z","iopub.status.idle":"2025-04-01T12:53:46.697029Z","shell.execute_reply.started":"2025-04-01T12:53:46.677541Z","shell.execute_reply":"2025-04-01T12:53:46.696085Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def save_checkpoint(model, optimizer, epoch, filename=\"resnet_checkpoint.pth\"):\n    torch.save({\"epoch\": epoch,\n                \"model_state\": model.state_dict(),\n                \"optimizer_state\": optimizer.state_dict()}, filename)\n    print(f\"Checkpoint saved to {filename}\")\n\ndef load_checkpoint(model, optimizer, filename=\"resnet_checkpoint.pth\"):\n    if os.path.isfile(filename):\n        checkpoint = torch.load(filename)\n        model.load_state_dict(checkpoint[\"model_state\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n        start_epoch = checkpoint[\"epoch\"] + 1\n        print(f\"Checkpoint loaded from {filename}, resuming at epoch {start_epoch}\")\n        return start_epoch\n    print(\"No checkpoint found, starting from scratch.\")\n    return 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T12:53:46.698008Z","iopub.execute_input":"2025-04-01T12:53:46.698300Z","iopub.status.idle":"2025-04-01T12:53:46.724408Z","shell.execute_reply.started":"2025-04-01T12:53:46.698274Z","shell.execute_reply":"2025-04-01T12:53:46.723310Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def main():\n    total_chunks = 14\n    num_epochs = 5\n    batch_size = 32\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Folders in /kaggle/input/ (update folder names as needed)\n    orig_folder = \"/kaggle/input/original_images\"\n    rec_folder = \"/kaggle/input/reconstructed_images\"\n    graph_folder = \"/kaggle/input/graph_images\"\n    \n    # Get list of image files from each folder.\n    orig_files_all = get_image_files(orig_folder)\n    rec_files_all = get_image_files(rec_folder)\n    graph_files_all = get_image_files(graph_folder)\n    \n    transform = ImageStatsTransform()\n    model = ResNetWithChannelFPN(num_classes=2).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n    criterion = nn.CrossEntropyLoss()\n    \n    start_epoch = load_checkpoint(model, optimizer, filename=\"resnet_checkpoint.pth\")\n    for epoch in range(start_epoch, num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        # Loop through 14 chunks.\n        for chunk in range(total_chunks):\n            print(f\"  Processing chunk {chunk+1}/{total_chunks}\")\n            # Get the chunk of file names.\n            orig_chunk = get_chunk(orig_files_all, chunk, total_chunks)\n            rec_chunk = get_chunk(rec_files_all, chunk, total_chunks)\n            graph_chunk = get_chunk(graph_files_all, chunk, total_chunks)\n            dataset = QuarkGluonDataset(orig_chunk, rec_chunk, graph_chunk, transform=transform)\n            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n            model.train()\n            for batch in dataloader:\n                x_orig = batch[\"original\"].to(device)\n                x_rec = batch[\"reconstructed\"].to(device)\n                x_graph = batch[\"graph\"].to(device)\n                y = torch.tensor(batch[\"label\"]).to(device)\n                optimizer.zero_grad()\n                outputs = model(x_orig, x_rec, x_graph)\n                loss = criterion(outputs, y)\n                loss.backward()\n                optimizer.step()\n            torch.cuda.empty_cache()\n        save_checkpoint(model, optimizer, epoch, filename=\"resnet_checkpoint.pth\")\n        print(f\"Epoch {epoch+1} complete.\")\n    print(\"Training complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T12:53:46.725503Z","iopub.execute_input":"2025-04-01T12:53:46.725866Z","iopub.status.idle":"2025-04-01T12:53:46.746856Z","shell.execute_reply.started":"2025-04-01T12:53:46.725825Z","shell.execute_reply":"2025-04-01T12:53:46.745876Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T12:53:46.747750Z","iopub.execute_input":"2025-04-01T12:53:46.748070Z","iopub.status.idle":"2025-04-01T12:53:59.784873Z","shell.execute_reply.started":"2025-04-01T12:53:46.748037Z","shell.execute_reply":"2025-04-01T12:53:59.783353Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 166MB/s]\n","output_type":"stream"},{"name":"stdout","text":"No checkpoint found, starting from scratch.\nEpoch 1/5\n  Loading chunk 1/14\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-9-238e0c94b620>:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  y = torch.tensor(batch[\"label\"]).to(device)\n","output_type":"stream"},{"name":"stdout","text":"  Loading chunk 2/14\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-972361fa1b80>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-238e0c94b620>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_rec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0morig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":10}]}