{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, glob, cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport timm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_image_files(folder):\n    return sorted(glob.glob(os.path.join(folder, \"*.*\")))\n\ndef get_chunk(file_list, chunk_index, total_chunks=14):\n    n = len(file_list)\n    chunk_size = n // total_chunks\n    start = chunk_index * chunk_size\n    end = n if chunk_index == total_chunks - 1 else start + chunk_size\n    return file_list[start:end]\n\ndef read_image(path, target_size=(32,32)):\n    img = cv2.imread(path)\n    if img is None:\n        raise FileNotFoundError(f\"Image not found: {path}\")\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, target_size)\n    return img.astype(np.float32) / 255.0\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ImageStatsTransform:\n    def __call__(self, x):\n        mean = x.mean(dim=[1,2], keepdim=True)\n        std = x.std(dim=[1,2], keepdim=True) + 1e-6\n        return (x - mean) / std\n\ndef compute_physics_features_tensor(x, eps=1e-6):\n    ecal = x[:, 0:1, :, :]\n    hcal = x[:, 1:2, :, :]\n    tracks = x[:, 2:3, :, :]\n    ratio = torch.mean(ecal / (hcal + eps), dim=[2,3])\n    mean_tracks = torch.mean(tracks, dim=[2,3])\n    diff = torch.mean(ecal - hcal, dim=[2,3])\n    norm_diff = torch.mean(torch.abs(ecal - hcal) / (ecal + hcal + eps), dim=[2,3])\n    return torch.cat([ratio, mean_tracks, diff, norm_diff], dim=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ChannelWiseFPN(nn.Module):\n    def __init__(self, in_channels=3, out_channels=16):\n        super(ChannelWiseFPN, self).__init__()\n        self.conv1x1 = nn.Conv2d(1, out_channels, kernel_size=1, padding=0)\n        self.conv3x3 = nn.Conv2d(1, out_channels, kernel_size=3, padding=1)\n        self.conv5x5 = nn.Conv2d(1, out_channels, kernel_size=5, padding=2)\n        self.fuse_conv = nn.Sequential(\n            nn.Conv2d(3*out_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(out_channels, out_channels, kernel_size=1)\n        )\n    def forward(self, x):\n        outputs = []\n        for i in range(x.shape[1]):\n            xi = x[:, i:i+1, :, :]\n            feat1 = self.conv1x1(xi)\n            feat2 = self.conv3x3(xi)\n            feat3 = self.conv5x5(xi)\n            cat_feats = torch.cat([feat1, feat2, feat3], dim=1)\n            fused = self.fuse_conv(cat_feats)\n            outputs.append(fused)\n        return torch.cat(outputs, dim=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class QuarkGluonDataset(Dataset):\n    def __init__(self, orig_files, rec_files, graph_files, transform=None):\n        self.orig_files = orig_files\n        self.rec_files = rec_files\n        self.graph_files = graph_files\n        self.transform = transform\n        self.labels = np.random.randint(0,2,size=(len(orig_files),))\n    def __len__(self):\n        return len(self.labels)\n    def __getitem__(self, idx):\n        orig = read_image(self.orig_files[idx])\n        rec = read_image(self.rec_files[idx])\n        graph = read_image(self.graph_files[idx])\n        orig = torch.tensor(orig).permute(2,0,1)\n        rec = torch.tensor(rec).permute(2,0,1)\n        graph = torch.tensor(graph).permute(2,0,1)\n        if self.transform:\n            orig = self.transform(orig)\n            rec = self.transform(rec)\n            graph = self.transform(graph)\n        label = self.labels[idx]\n        return {\"original\": orig, \"reconstructed\": rec, \"graph\": graph, \"label\": label}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ViTWithChannelFPN(nn.Module):\n    def __init__(self, num_classes=2):\n        super(ViTWithChannelFPN, self).__init__()\n        self.fpn_orig = ChannelWiseFPN(3, 16)\n        self.fpn_diff = ChannelWiseFPN(3, 16)\n        self.fpn_graph = ChannelWiseFPN(3, 16)\n        self.vit_orig = timm.create_model('vit_base_patch16_224', pretrained=True)\n        self.vit_diff = timm.create_model('vit_base_patch16_224', pretrained=True)\n        self.vit_graph = timm.create_model('vit_base_patch16_224', pretrained=True)\n        for net in [self.vit_orig, self.vit_diff, self.vit_graph]:\n            net.patch_embed.proj = nn.Conv2d(48, net.patch_embed.proj.out_channels,\n                                             kernel_size=net.patch_embed.proj.kernel_size,\n                                             stride=net.patch_embed.proj.stride,\n                                             padding=net.patch_embed.proj.padding)\n            net.head = nn.Identity()\n        self.classifier = nn.Linear(768 + 4, num_classes)\n    def forward(self, x_orig, x_rec, x_graph):\n        x_diff = x_orig - x_rec\n        feat_orig = self.vit_orig(self.fpn_orig(x_orig))\n        feat_diff = self.vit_diff(self.fpn_diff(x_diff))\n        feat_graph = self.vit_graph(self.fpn_graph(x_graph))\n        deep_feat = (feat_orig + feat_diff + feat_graph) / 3.0\n        phys_feat = compute_physics_features_tensor(x_orig)\n        combined_feat = torch.cat([deep_feat, phys_feat], dim=1)\n        logits = self.classifier(combined_feat)\n        return logits","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_checkpoint(model, optimizer, epoch, filename=\"vit_checkpoint.pth\"):\n    torch.save({\"epoch\": epoch,\n                \"model_state\": model.state_dict(),\n                \"optimizer_state\": optimizer.state_dict()}, filename)\n    print(f\"Checkpoint saved to {filename}\")\n\ndef load_checkpoint(model, optimizer, filename=\"vit_checkpoint.pth\"):\n    if os.path.isfile(filename):\n        checkpoint = torch.load(filename)\n        model.load_state_dict(checkpoint[\"model_state\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n        start_epoch = checkpoint[\"epoch\"] + 1\n        print(f\"Checkpoint loaded from {filename}, resuming at epoch {start_epoch}\")\n        return start_epoch\n    print(\"No checkpoint found, starting from scratch.\")\n    return 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    total_chunks = 14\n    num_epochs = 5\n    batch_size = 32\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Update folder paths according to new dataset structure.\n    orig_folder = \"/kaggle/input/genie-extracted-dataset\"\n    rec_folders = [\n        \"/kaggle/input/genie-output-part-1/reconstructions\", \n        \"/kaggle/input/genie-common-task-1-output-part-2final/reconstructions\"\n    ]\n    graph_folders = [\n        \"/kaggle/input/common-task-2-dataset-part-1/processed_jet_graphs\",\n        \"/kaggle/input/output-part-2-of-task-2/processed_jet_graphs\",\n        \"/kaggle/input/output-part-3-of-task-2/processed_jet_graphs\",\n        \"/kaggle/input/part-4-task-2-output/processed_jet_graphs\"\n    ]\n    \n    # Get original image files.\n    orig_files_all = get_image_files(orig_folder)\n    \n    # Gather reconstructed image files from both parts.\n    rec_files_all = []\n    for folder in rec_folders:\n        rec_files_all.extend(get_image_files(folder))\n    \n    # Gather graph files from all parts, filtering out the specified files.\n    graph_files_all = []\n    for folder in graph_folders:\n        files = get_image_files(folder)\n        ignore_files = []\n        if \"common-task-2-dataset-part-1\" in folder:\n            ignore_files.append(\"processed_chunk_120000_130000.pt\")\n        if \"output-part-2-of-task-2\" in folder:\n            ignore_files.append(\"processed_chunk_40000_50000.pt\")\n        if \"output-part-3-of-task-2\" in folder:\n            ignore_files.append(\"processed_chunk_80000_90000.pt\")\n        filtered_files = [f for f in files if os.path.basename(f) not in ignore_files]\n        graph_files_all.extend(filtered_files)\n    \n    transform = ImageStatsTransform()\n    model = ResNetWithChannelFPN(num_classes=2).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n    criterion = nn.CrossEntropyLoss()\n    \n    start_epoch = load_checkpoint(model, optimizer, filename=\"resnet_checkpoint.pth\")\n    \n    for epoch in range(start_epoch, num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        # Process data in 14 chunks.\n        for chunk in range(total_chunks):\n            print(f\"  Processing chunk {chunk+1}/{total_chunks}\")\n            orig_chunk = get_chunk(orig_files_all, chunk, total_chunks)\n            rec_chunk = get_chunk(rec_files_all, chunk, total_chunks)\n            graph_chunk = get_chunk(graph_files_all, chunk, total_chunks)\n            \n            dataset = QuarkGluonDataset(orig_chunk, rec_chunk, graph_chunk, transform=transform)\n            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n            \n            model.train()\n            for batch in dataloader:\n                x_orig = batch[\"original\"].to(device)\n                x_rec = batch[\"reconstructed\"].to(device)\n                x_graph = batch[\"graph\"].to(device)\n                y = torch.tensor(batch[\"label\"]).to(device)\n                \n                optimizer.zero_grad()\n                outputs = model(x_orig, x_rec, x_graph)\n                loss = criterion(outputs, y)\n                loss.backward()\n                optimizer.step()\n            \n            torch.cuda.empty_cache()\n        \n        save_checkpoint(model, optimizer, epoch, filename=\"resnet_checkpoint.pth\")\n        print(f\"Epoch {epoch+1} complete.\")\n    \n    print(\"Training complete.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
