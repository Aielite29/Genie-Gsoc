{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11151123,"sourceType":"datasetVersion","datasetId":6957107}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport math\nimport shutil\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport random\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:12:26.822681Z","iopub.execute_input":"2025-03-26T14:12:26.822947Z","iopub.status.idle":"2025-03-26T14:12:29.847213Z","shell.execute_reply.started":"2025-03-26T14:12:26.822927Z","shell.execute_reply":"2025-03-26T14:12:29.846621Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from tqdm.notebook import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ChunkDataset(Dataset):\n    def __init__(self, npz_file, key='X_jets'):\n        print(f\"Loading chunk: {npz_file}\")\n        with np.load(npz_file) as data:\n            self.images = data[key]  # shape: (N,125,125,3)\n            # All other keys are considered properties.\n            self.props = {k: data[k] for k in data.files if k != key}\n        if self.images.max() > 1.0:\n            self.images = self.images.astype('float32') / 255.0\n        self.len = self.images.shape[0]\n    \n    def __len__(self):\n        return self.len\n    \n    def __getitem__(self, idx):\n        image = self.images[idx].astype('float32')\n        image = np.transpose(image, (2, 0, 1))  # (3,125,125)\n        properties = {k: self.props[k][idx] for k in self.props}\n        return torch.tensor(image, dtype=torch.float32), properties","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:12:29.848250Z","iopub.execute_input":"2025-03-26T14:12:29.848701Z","iopub.status.idle":"2025-03-26T14:12:29.854498Z","shell.execute_reply.started":"2025-03-26T14:12:29.848670Z","shell.execute_reply":"2025-03-26T14:12:29.853697Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class CosineAnnealingWarmupScheduler(_LRScheduler):\n    def __init__(self, optimizer, warmup_epochs, max_epochs, eta_min=0, last_epoch=-1):\n        self.warmup_epochs = warmup_epochs\n        self.max_epochs = max_epochs\n        self.eta_min = eta_min\n        super(CosineAnnealingWarmupScheduler, self).__init__(optimizer, last_epoch)\n    \n    def get_lr(self):\n        if self.last_epoch < self.warmup_epochs:\n            # Warmup phase: linearly scale from 0 to base_lr\n            return [base_lr * (self.last_epoch + 1) / self.warmup_epochs for base_lr in self.base_lrs]\n        else:\n            # Cosine annealing phase\n            cos_epoch = self.last_epoch - self.warmup_epochs\n            total_cos = self.max_epochs - self.warmup_epochs\n            return [self.eta_min + (base_lr - self.eta_min) *\n                    (1 + math.cos(math.pi * cos_epoch / total_cos)) / 2\n                    for base_lr in self.base_lrs]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:12:29.855725Z","iopub.execute_input":"2025-03-26T14:12:29.855911Z","iopub.status.idle":"2025-03-26T14:12:29.871053Z","shell.execute_reply.started":"2025-03-26T14:12:29.855895Z","shell.execute_reply":"2025-03-26T14:12:29.870490Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class ChannelEncoder(nn.Module):\n\n    def __init__(self, in_channels=1, base_channels=32):\n        super(ChannelEncoder, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels, base_channels, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(base_channels),\n            nn.ReLU(inplace=True)\n        )  # -> (B, base_channels, 63, 63)\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(base_channels, base_channels*2, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(base_channels*2),\n            nn.ReLU(inplace=True)\n        )  # -> (B, base_channels*2, 32, 32)\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(base_channels*2, base_channels*4, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(base_channels*4),\n            nn.ReLU(inplace=True)\n        )  # -> (B, base_channels*4, 16, 16)\n    \n    def forward(self, x):\n        skip1 = self.layer1(x)\n        skip2 = self.layer2(skip1)\n        latent = self.layer3(skip2)\n        return latent, skip1, skip2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:12:29.872130Z","iopub.execute_input":"2025-03-26T14:12:29.872377Z","iopub.status.idle":"2025-03-26T14:12:29.889158Z","shell.execute_reply.started":"2025-03-26T14:12:29.872332Z","shell.execute_reply":"2025-03-26T14:12:29.888412Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class SelfAttention(nn.Module):\n\n    def __init__(self, in_channels):\n        super(SelfAttention, self).__init__()\n        self.query = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n        self.key   = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n    \n    def forward(self, x):\n        B, C, H, W = x.shape\n        proj_query = self.query(x).view(B, -1, H*W)\n        proj_key   = self.key(x).view(B, -1, H*W)\n        energy = torch.bmm(proj_query.permute(0, 2, 1), proj_key)\n        attention = F.softmax(energy, dim=-1)\n        proj_value = self.value(x).view(B, -1, H*W)\n        out = torch.bmm(proj_value, attention.permute(0, 2, 1)).view(B, C, H, W)\n        return self.gamma * out + x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:12:29.889971Z","iopub.execute_input":"2025-03-26T14:12:29.890185Z","iopub.status.idle":"2025-03-26T14:12:29.905899Z","shell.execute_reply.started":"2025-03-26T14:12:29.890168Z","shell.execute_reply":"2025-03-26T14:12:29.905305Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class CrossChannelFusion(nn.Module):\n \n    def __init__(self, in_channels, out_channels):\n        super(CrossChannelFusion, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.attn = SelfAttention(out_channels)\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.attn(x)\n        return x\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:12:29.906580Z","iopub.execute_input":"2025-03-26T14:12:29.906814Z","iopub.status.idle":"2025-03-26T14:12:29.922309Z","shell.execute_reply.started":"2025-03-26T14:12:29.906796Z","shell.execute_reply":"2025-03-26T14:12:29.921774Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class Decoder(nn.Module):\n \n    def __init__(self, base_channels=32):\n        super(Decoder, self).__init__()\n        self.up1 = nn.Sequential(\n            nn.ConvTranspose2d(base_channels*8, base_channels*4, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.BatchNorm2d(base_channels*4),\n            nn.ReLU(inplace=True)\n        )  # -> (B, base_channels*4, 32,32)\n        \n        self.up2 = nn.Sequential(\n            nn.ConvTranspose2d(base_channels*(4 + 2*3), base_channels*2, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.BatchNorm2d(base_channels*2),\n            nn.ReLU(inplace=True)\n        )  # -> (B, base_channels*2, ~63,63)\n        \n        self.up3 = nn.Sequential(\n            nn.ConvTranspose2d(base_channels*(2 + 3), base_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.BatchNorm2d(base_channels),\n            nn.ReLU(inplace=True)\n        )  # -> (B, base_channels, ~125,125)\n        \n        self.up4 = nn.Sequential(\n            nn.ConvTranspose2d(base_channels, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.Sigmoid()\n        )  # -> (B, 3, 250,250)\n    \n    def forward(self, latent, skips_e, skips_h, skips_t):\n        x = self.up1(latent)  # (B, base_channels*4, 32,32)\n        skip2_fused = torch.cat([skips_e[1], skips_h[1], skips_t[1]], dim=1)  # (B, base_channels*6,32,32)\n        x = torch.cat([x, skip2_fused], dim=1)  # (B, base_channels*10,32,32)\n        x = self.up2(x)  # (B, base_channels*2, ~H, ~W)  Expected: ~ (B, base_channels*2, 64,64) or 63x63\n        \n        # Fuse skip1 features: they are expected to be (B, base_channels, 63,63)\n        skip1_fused = torch.cat([skips_e[0], skips_h[0], skips_t[0]], dim=1)  # (B, base_channels*3, 63,63)\n        # If spatial dimensions do not match, interpolate skip1_fused to match x\n        if x.shape[2] != skip1_fused.shape[2] or x.shape[3] != skip1_fused.shape[3]:\n            skip1_fused = F.interpolate(skip1_fused, size=(x.shape[2], x.shape[3]), mode='bilinear', align_corners=False)\n        x = torch.cat([x, skip1_fused], dim=1)  # (B, base_channels*2 + base_channels*3, H, W)\n        x = self.up3(x)  # (B, base_channels, ~125,125)\n        x = self.up4(x)  # (B, 3, 250,250)\n        x = F.interpolate(x, size=(125,125), mode='bilinear', align_corners=False)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:12:29.923209Z","iopub.execute_input":"2025-03-26T14:12:29.923474Z","iopub.status.idle":"2025-03-26T14:12:29.937796Z","shell.execute_reply.started":"2025-03-26T14:12:29.923445Z","shell.execute_reply":"2025-03-26T14:12:29.937142Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class PhysicsInformedAutoencoder(nn.Module):\n    def __init__(self, base_channels=32):\n        super(PhysicsInformedAutoencoder, self).__init__()\n        self.encoder_e = ChannelEncoder(1, base_channels)\n        self.encoder_h = ChannelEncoder(1, base_channels)\n        self.encoder_t = ChannelEncoder(1, base_channels)\n        # Each encoder latent: (B, base_channels*4,16,16) → concatenated: (B, base_channels*12,16,16)\n        self.fusion = CrossChannelFusion(base_channels*12, base_channels*8)\n        self.decoder = Decoder(base_channels)\n    \n    def forward(self, x):\n        # x: (B,3,125,125)\n        x_e = x[:,0:1,:,:]  # ECAL-like\n        x_h = x[:,1:2,:,:]  # HCAL-like\n        x_t = x[:,2:3,:,:]  # Tracks\n        \n        latent_e, se1, se2 = self.encoder_e(x_e)\n        latent_h, sh1, sh2 = self.encoder_h(x_h)\n        latent_t, st1, st2 = self.encoder_t(x_t)\n        \n        skips_e = (se1, se2)\n        skips_h = (sh1, sh2)\n        skips_t = (st1, st2)\n        \n        latent_cat = torch.cat([latent_e, latent_h, latent_t], dim=1)  # (B, base_channels*12,16,16)\n        fused_latent = self.fusion(latent_cat)  # (B, base_channels*8,16,16)\n        out = self.decoder(fused_latent, skips_e, skips_h, skips_t)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:12:29.938552Z","iopub.execute_input":"2025-03-26T14:12:29.938729Z","iopub.status.idle":"2025-03-26T14:12:29.955045Z","shell.execute_reply.started":"2025-03-26T14:12:29.938714Z","shell.execute_reply":"2025-03-26T14:12:29.954415Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def physics_informed_loss(output, target, alpha=0.7):\n\n    mse_loss = F.mse_loss(output, target)\n    energy_out = output.sum(dim=[1,2,3])\n    energy_target = target.sum(dim=[1,2,3])\n    energy_loss = F.l1_loss(energy_out, energy_target)\n    total_loss = alpha * energy_loss + (1 - alpha) * mse_loss\n    return total_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:12:29.956861Z","iopub.execute_input":"2025-03-26T14:12:29.957047Z","iopub.status.idle":"2025-03-26T14:12:29.971960Z","shell.execute_reply.started":"2025-03-26T14:12:29.957031Z","shell.execute_reply":"2025-03-26T14:12:29.971323Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def train_on_chunk(model, train_loader, optimizer, device, alpha=0.7):\n    model.train()\n    running_loss = 0.0\n    for batch in train_loader:\n        images, _ = batch\n        images = images.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = physics_informed_loss(outputs, images, alpha=alpha)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * images.size(0)\n    return running_loss / len(train_loader.dataset)\n\ndef eval_on_chunk(model, val_loader, device, alpha=0.7):\n    model.eval()\n    total_loss = 0.0\n    with torch.no_grad():\n        for batch in val_loader:\n            images, _ = batch\n            images = images.to(device)\n            outputs = model(images)\n            loss = physics_informed_loss(outputs, images, alpha=alpha)\n            total_loss += loss.item() * images.size(0)\n    return total_loss / len(val_loader.dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:12:29.973002Z","iopub.execute_input":"2025-03-26T14:12:29.973268Z","iopub.status.idle":"2025-03-26T14:12:29.985817Z","shell.execute_reply.started":"2025-03-26T14:12:29.973241Z","shell.execute_reply":"2025-03-26T14:12:29.985208Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def chunk_based_training(\n    chunk_dir,\n    chunk_files,\n    model,\n    optimizer,\n    scheduler,\n    device,\n    epochs=5,\n    alpha_loss=0.7,\n    train_ratio=0.8,\n    batch_size=32,\n    num_workers=2\n):\n    for epoch in range(epochs):\n        epoch_train_loss = 0.0\n        epoch_val_loss = 0.0\n        total_train_samples = 0\n        total_val_samples = 0\n        \n        print(f\"\\n=== Epoch {epoch+1}/{epochs} ===\")\n        for chunk_file in chunk_files:\n            chunk_path = os.path.join(chunk_dir, chunk_file)\n            dataset = ChunkDataset(chunk_path, key='X_jets')\n            n_total = len(dataset)\n            n_train = int(train_ratio * n_total)\n            n_val = n_total - n_train\n            train_ds, val_ds = random_split(dataset, [n_train, n_val])\n            \n            train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n            val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n            \n            train_loss = train_on_chunk(model, train_loader, optimizer, device, alpha=alpha_loss)\n            val_loss = eval_on_chunk(model, val_loader, device, alpha=alpha_loss)\n            \n            epoch_train_loss += train_loss * n_train\n            epoch_val_loss += val_loss * n_val\n            total_train_samples += n_train\n            total_val_samples += n_val\n            \n            del dataset, train_ds, val_ds, train_loader, val_loader\n            gc.collect()\n        \n        epoch_train_loss /= total_train_samples\n        epoch_val_loss /= total_val_samples\n        scheduler.step()\n        print(f\"Epoch {epoch+1} - Train Loss: {epoch_train_loss:.6f} | Val Loss: {epoch_val_loss:.6f}\")\n    print(\"Training complete after all epochs.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:12:29.986602Z","iopub.execute_input":"2025-03-26T14:12:29.986855Z","iopub.status.idle":"2025-03-26T14:12:30.005623Z","shell.execute_reply.started":"2025-03-26T14:12:29.986826Z","shell.execute_reply":"2025-03-26T14:12:30.004823Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def save_reconstructed_chunk(chunk_file, model, output_folder, device, batch_size=32, key='X_jets'):\n    chunk_name = os.path.splitext(os.path.basename(chunk_file))[0]\n    dataset = ChunkDataset(chunk_file, key=key)\n    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    model.eval()\n    reconstructions = []\n    with torch.no_grad():\n        for batch in tqdm(data_loader, desc=f\"Inferencing {chunk_name}\"):\n            images, _ = batch\n            images = images.to(device)\n            output = model(images)  # (B,3,125,125)\n            reconstructions.append(output.cpu().numpy())\n    recon_all = np.concatenate(reconstructions, axis=0)  # (N,3,125,125)\n    recon_all = np.transpose(recon_all, (0,2,3,1))       # (N,125,125,3)\n    \n    os.makedirs(output_folder, exist_ok=True)\n    npz_save_path = os.path.join(output_folder, f\"{chunk_name}_recon.npz\")\n    np.savez_compressed(npz_save_path, X_recon=recon_all)\n    print(f\"Saved reconstructed chunk to: {npz_save_path}\")\n    \n    zip_base = os.path.splitext(npz_save_path)[0]\n    zip_path = shutil.make_archive(zip_base, 'zip', output_folder, os.path.basename(npz_save_path))\n    print(f\"Zipped reconstructed chunk: {zip_path}\")\n    os.remove(npz_save_path)\n    print(\"Removed unzipped npz file to save space.\")\n    \n    del dataset, data_loader, reconstructions, recon_all\n    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:12:30.006564Z","iopub.execute_input":"2025-03-26T14:12:30.006787Z","iopub.status.idle":"2025-03-26T14:12:30.025319Z","shell.execute_reply.started":"2025-03-26T14:12:30.006758Z","shell.execute_reply":"2025-03-26T14:12:30.024718Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def visualize_reconstructions_chunk(chunk_file, model, device, num_samples=10, key='X_jets'):\n    dataset = ChunkDataset(chunk_file, key=key)\n    data_loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n    model.eval()\n    originals_list = []\n    recon_list = []\n    with torch.no_grad():\n        for i, batch in enumerate(data_loader):\n            if i >= num_samples:\n                break\n            images, _ = batch\n            images = images.to(device)\n            output = model(images)  # (1,3,125,125)\n            originals_list.append(images.cpu().squeeze(0))\n            recon_list.append(output.cpu().squeeze(0))\n    plt.figure(figsize=(20, num_samples*2))\n    for i in range(num_samples):\n        plt.subplot(num_samples, 2, 2*i+1)\n        plt.imshow(originals_list[i].permute(1,2,0).numpy())\n        plt.title(\"Original\")\n        plt.axis('off')\n        plt.subplot(num_samples, 2, 2*i+2)\n        plt.imshow(recon_list[i].permute(1,2,0).numpy())\n        plt.title(\"Reconstructed\")\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:12:30.026063Z","iopub.execute_input":"2025-03-26T14:12:30.026323Z","iopub.status.idle":"2025-03-26T14:12:30.042159Z","shell.execute_reply.started":"2025-03-26T14:12:30.026298Z","shell.execute_reply":"2025-03-26T14:12:30.041427Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Define your chunk directory and get list of .npz chunk files\n    chunk_dir = \"/kaggle/input/genie-extracted-dataset\"\n    chunk_files = sorted([f for f in os.listdir(chunk_dir) if f.endswith(\".npz\") and f.startswith(\"chunk_\")])\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = PhysicsInformedAutoencoder(base_channels=32).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    scheduler = CosineAnnealingWarmupScheduler(optimizer, warmup_epochs=5, max_epochs=20, eta_min=1e-5)\n    \n    epochs = 20\n    alpha_loss = 0.7\n    train_ratio = 0.9\n    batch_size = 100\n    num_workers = 2\n    \n    # Train model epoch-by-epoch, processing each chunk sequentially.\n    chunk_based_training(\n        chunk_dir=chunk_dir,\n        chunk_files=chunk_files,\n        model=model,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        device=device,\n        epochs=epochs,\n        alpha_loss=alpha_loss,\n        train_ratio=train_ratio,\n        batch_size=batch_size,\n        num_workers=num_workers\n    )\n    \n    # After training, run inference on all chunks except the last three\n    if len(chunk_files) > 3:\n        inference_files = chunk_files[:-3]\n    else:\n        inference_files = chunk_files\n    \n    output_folder = \"/kaggle/working/reconstructions\"\n    for cf in inference_files:\n        chunk_path = os.path.join(chunk_dir, cf)\n        save_reconstructed_chunk(chunk_path, model, output_folder=output_folder, device=device, batch_size=batch_size)\n    \n    # Save final model weights\n    final_model_path = \"final_model.pth\"\n    torch.save(model.state_dict(), final_model_path)\n    print(f\"Model saved to {final_model_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:23:20.615633Z","iopub.execute_input":"2025-03-26T14:23:20.615925Z","execution_failed":"2025-03-26T14:38:41.290Z"}},"outputs":[{"name":"stdout","text":"\n=== Epoch 1/20 ===\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_0_10000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_100000_110000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_10000_20000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_110000_120000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_120000_130000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_130000_139306.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_20000_30000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_30000_40000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_40000_50000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_50000_60000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_60000_70000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_70000_80000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_80000_90000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_90000_100000.npz\nEpoch 1 - Train Loss: 3023.803035 | Val Loss: 2446.116206\n\n=== Epoch 2/20 ===\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_0_10000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_100000_110000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_10000_20000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_110000_120000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_120000_130000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_130000_139306.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_20000_30000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_30000_40000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_40000_50000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_50000_60000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_60000_70000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_70000_80000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_80000_90000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_90000_100000.npz\nEpoch 2 - Train Loss: 122.952400 | Val Loss: 16.223768\n\n=== Epoch 3/20 ===\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_0_10000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_100000_110000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_10000_20000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_110000_120000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_120000_130000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_130000_139306.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_20000_30000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_30000_40000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_40000_50000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_50000_60000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_60000_70000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_70000_80000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_80000_90000.npz\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_90000_100000.npz\nEpoch 3 - Train Loss: 19.001177 | Val Loss: 17.753854\n\n=== Epoch 4/20 ===\nLoading chunk: /kaggle/input/genie-extracted-dataset/chunk_0_10000.npz\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
